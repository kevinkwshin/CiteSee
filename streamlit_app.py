import streamlit as st
import pandas as pd
from scholarly import scholarly
import io
import requests
import time
import json # JSONì„ ì˜ˆì˜ê²Œ ì¶œë ¥í•˜ê¸° ìœ„í•´ ì¶”ê°€

# ----------------------------------
# í˜ì´ì§€ ì„¤ì •
# ----------------------------------
st.set_page_config(
    page_title="Scholar ê²€ìƒ‰ê¸° (S2 API ì—°ë™)",
    page_icon="ğŸ› ï¸",
    layout="wide",
)

# ----------------------------------
# Semantic Scholar API ì—°ë™ í•¨ìˆ˜ (ë¡œê·¸ ê¸°ëŠ¥ ê°•í™”)
# ----------------------------------
@st.cache_data(ttl=3600)
def get_journal_info_from_s2(journal_name: str):
    """
    S2 APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ë„ ì •ë³´ë¥¼ ì¡°íšŒí•˜ê³ , ëª¨ë“  ê³¼ì •ì„ ë¡œê·¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    :return: ((ê²°ê³¼ íŠœí”Œ), (ë¡œê·¸ ë”•ì…”ë„ˆë¦¬))
    """
    log_info = {
        "input_journal": journal_name,
        "request_url": "N/A",
        "status_code": "N/A",
        "raw_response": "N/A",
        "error_message": "N/A",
    }
    
    if not journal_name:
        log_info["error_message"] = "ì…ë ¥ëœ ì €ë„ëª…ì´ ì—†ìŠµë‹ˆë‹¤."
        return (("N/A", "#", "N/A"), log_info)

    try:
        params = {"query": journal_name, "fields": "journalName,homepage,influenceScore"}
        response = requests.get(
            "https://api.semanticscholar.org/graph/v1/journal/search",
            params=params,
            headers={"Accept": "application/json"},
            timeout=15
        )
        
        # --- ëª¨ë“  ì‘ë‹µ ì •ë³´ë¥¼ ë¡œê·¸ì— ê¸°ë¡ ---
        log_info["request_url"] = response.url # ìµœì¢…ì ìœ¼ë¡œ ìš”ì²­ëœ URL
        log_info["status_code"] = response.status_code
        log_info["raw_response"] = response.text

        response.raise_for_status() # 200ë²ˆëŒ€ ì½”ë“œê°€ ì•„ë‹ˆë©´ ì—¬ê¸°ì„œ ì—ëŸ¬ ë°œìƒì‹œí‚´
        
        data = response.json()

        if data.get("total", 0) > 0 and data.get("data"):
            journal_data = data["data"][0]
            influence_score = journal_data.get("influenceScore")
            score_str = f"{float(influence_score):.2f}" if influence_score is not None else "N/A"
            homepage = journal_data.get("homepage") or "#"
            s2_journal_name = journal_data.get("journalName") or "N/A"
            return ((score_str, homepage, s2_journal_name), log_info)
        else:
            log_info["error_message"] = "APIëŠ” ì„±ê³µí–ˆìœ¼ë‚˜, ì‘ë‹µ ë°ì´í„°ì— ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            return (("N/A", "#", "ê²°ê³¼ ì—†ìŒ"), log_info)
            
    except requests.exceptions.HTTPError as http_err:
        log_info["error_message"] = f"HTTP ì—ëŸ¬ ë°œìƒ: {http_err}"
    except requests.exceptions.RequestException as req_err:
        log_info["error_message"] = f"ë„¤íŠ¸ì›Œí¬/ìš”ì²­ ì—ëŸ¬ ë°œìƒ: {req_err}"
    except Exception as e:
        log_info["error_message"] = f"ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬ ë°œìƒ: {e}"

    return (("API ì˜¤ë¥˜", "#", "N/A"), log_info)


# ----------------------------------
# ë°ì´í„° ë³€í™˜ í•¨ìˆ˜
# ----------------------------------
@st.cache_data
def to_excel(df: pd.DataFrame):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Sheet1')
    return output.getvalue()

@st.cache_data
def to_csv(df: pd.DataFrame):
    return df.to_csv(index=False).encode('utf-8-sig')

# ----------------------------------
# Streamlit ì•± UI êµ¬ì„±
# ----------------------------------
st.title("ğŸ› ï¸ Scholar ê²€ìƒ‰ê¸° (API ë””ë²„ê±° í¬í•¨)")
st.markdown("Google Scholar ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³ , Semantic Scholar APIë¡œ ì €ë„ ì •ë³´ë¥¼ ì‹¤ì‹œê°„ ì¡°íšŒí•©ë‹ˆë‹¤.")

with st.form(key='search_form'):
    keyword = st.text_input("**ğŸ‘‰ ê²€ìƒ‰ì–´(Keyword)ë¥¼ ì…ë ¥í•˜ì„¸ìš”**", placeholder="ì˜ˆ: large language models")
    num_results = st.number_input("**ğŸ‘‰ ê²€ìƒ‰í•  ë…¼ë¬¸ ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”**", min_value=1, max_value=20, value=5, step=1)
    submit_button = st.form_submit_button(label='ğŸ” ê²€ìƒ‰ ì‹œì‘')

if submit_button and keyword:
    with st.spinner(f"'{keyword}' ë…¼ë¬¸ ê²€ìƒ‰ ë° S2 APIë¡œ ì €ë„ ì •ë³´ ì¡°íšŒ ì¤‘..."):
        search_query = scholarly.search_pubs(keyword)
        results = []
        api_logs = [] # API ë¡œê·¸ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

        for i, pub in enumerate(search_query):
            if i >= num_results: break
            time.sleep(0.5)
            bib = pub.get('bib', {})
            venue = bib.get('venue', 'N/A')
            
            # API í˜¸ì¶œ ë° ê²°ê³¼ì™€ ë¡œê·¸ë¥¼ í•¨ê»˜ ë°›ìŒ
            (api_result, log_info) = get_journal_info_from_s2(venue)
            influence_score, journal_homepage, s2_name = api_result
            api_logs.append(log_info) # ë¡œê·¸ ì €ì¥
            
            results.append({
                "ì œëª© (Title)": bib.get('title', 'N/A'),
                "ì €ì (Authors)": ", ".join(bib.get('author', ['N/A'])),
                "ì—°ë„ (Year)": bib.get('pub_year', 'N/A'),
                "ì €ë„/ì¶œíŒë¬¼ (Venue)": venue,
                "ì˜í–¥ë ¥ ì ìˆ˜ (S2)": influence_score,
                "í”¼ì¸ìš© ìˆ˜ (Citations)": pub.get('num_citations', 0),
                "ë…¼ë¬¸ ë§í¬": pub.get('pub_url', '#'),
                "ì €ë„ í™ˆí˜ì´ì§€": journal_homepage,
                "S2 ì €ë„ëª…": s2_name,
            })
        
        st.success(f"ì´ {len(results)}ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        df = pd.DataFrame(results)
        st.dataframe(
            df, use_container_width=True,
            column_config={
                "ë…¼ë¬¸ ë§í¬": st.column_config.LinkColumn("ë°”ë¡œê°€ê¸°"),
                "ì €ë„ í™ˆí˜ì´ì§€": st.column_config.LinkColumn("ë°”ë¡œê°€ê¸°")
            }, hide_index=True)
        
        st.markdown("---")
        st.subheader("ğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ")
        col1, col2 = st.columns(2)
        # ... (ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì½”ë“œëŠ” ë™ì¼)
        
        # --- ìƒì„¸ ë¡œê·¸ í‘œì‹œ ì„¹ì…˜ ---
        with st.expander("ğŸ” API ìš”ì²­ ìƒì„¸ ë¡œê·¸ ë³´ê¸° (ë¬¸ì œ í•´ê²°ìš©)"):
            if not api_logs:
                st.info("ê¸°ë¡ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                for idx, log in enumerate(api_logs):
                    st.markdown(f"---")
                    st.markdown(f"**#{idx+1} ìš”ì²­**")
                    st.markdown(f"**- ìš”ì²­ ì €ë„ëª…:** `{log['input_journal']}`")
                    st.markdown(f"**- ìƒíƒœ ì½”ë“œ:** `{log['status_code']}`")
                    
                    if log['error_message'] != "N/A":
                        st.error(f"**- ì˜¤ë¥˜ ë©”ì‹œì§€:** {log['error_message']}")

                    st.markdown("**- ì „ì²´ ìš”ì²­ URL:**")
                    st.code(log['request_url'], language='text')

                    st.markdown("**- ì„œë²„ ì›ë³¸ ì‘ë‹µ:**")
                    try:
                        # JSON í˜•ì‹ì´ë¼ë©´ ì˜ˆì˜ê²Œ ì¶œë ¥
                        pretty_json = json.dumps(json.loads(log['raw_response']), indent=2)
                        st.code(pretty_json, language='json')
                    except (json.JSONDecodeError, TypeError):
                        # JSONì´ ì•„ë‹ˆë©´ ê·¸ëƒ¥ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥
                        st.code(log['raw_response'], language='text')

elif submit_button and not keyword:
    st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
