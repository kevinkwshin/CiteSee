import streamlit as st
import pandas as pd
from scholarly import scholarly
import io
import requests
from bs4 import BeautifulSoup
import re
import time

# --- 1. í˜ì´ì§€ ì„¤ì • ë° ìƒìˆ˜ ì •ì˜ ---
st.set_page_config(
    page_title="ë…¼ë¬¸ ê²€ìƒ‰ê¸°",
    page_icon="ğŸ¯",
    layout="centered", # ì‹¬í”Œí•œ UIë¥¼ ìœ„í•œ ë ˆì´ì•„ì›ƒ
)

# High Impact Journalì„ íŒë‹¨í•˜ëŠ” IF ì„ê³„ê°’ (ì´ ê°’ë³´ë‹¤ ë†’ìœ¼ë©´ 'Y')
# ì´ ê¸°ì¤€ì€ ë¶„ì•¼ë§ˆë‹¤ ë‹¤ë¥´ë¯€ë¡œ, í•„ìš”ì— ë”°ë¼ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
HIGH_IMPACT_THRESHOLD = 10.0

# --- 2. í•µì‹¬ í•¨ìˆ˜: Google ê²€ìƒ‰ìœ¼ë¡œ Y/N íŒë³„ ---
@st.cache_data(ttl=3600) # 1ì‹œê°„ ë™ì•ˆ ê²€ìƒ‰ ê²°ê³¼ ìºì‹±
def check_if_high_impact(journal_name: str):
    """
    Google ê²€ìƒ‰ìœ¼ë¡œ ì €ë„ì˜ IFë¥¼ ì°¾ì•„, ì„ê³„ê°’ê³¼ ë¹„êµí•˜ì—¬ Y/Nì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    **ë§¤ìš° ë¶ˆì•ˆì •í•˜ë©° ì‹¤í—˜ì ì¸ ê¸°ëŠ¥ì…ë‹ˆë‹¤.**
    """
    if not journal_name:
        return "N/A"

    try:
        query = f'"{journal_name}" journal impact factor'
        url = f"https://www.google.com/search?q={query}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        
        if "Our systems have detected unusual traffic" in response.text:
            return "ì°¨ë‹¨ë¨"
        
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        page_text = soup.get_text()
        
        # IFë¡œ ì¶”ì •ë˜ëŠ” ìˆ«ì íŒ¨í„´ ì°¾ê¸°
        pattern = r"(?:impact factor|if)\s*[:\-]?\s*(\d{1,3}\.\d{1,3})"
        match = re.search(pattern, page_text, re.IGNORECASE)
        
        if match:
            extracted_if = float(match.group(1))
            # ì„ê³„ê°’ê³¼ ë¹„êµí•˜ì—¬ Y/N ë°˜í™˜
            return "Y" if extracted_if >= HIGH_IMPACT_THRESHOLD else "N"
        else:
            return "N/A" # IF ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•¨

    except Exception:
        return "ì˜¤ë¥˜"

# --- 3. ë°ì´í„° ë³€í™˜ í•¨ìˆ˜ ---
@st.cache_data
def convert_to_csv(df):
    return df.to_csv(index=False).encode('utf-8-sig')

# --- 4. UI ë³¸ë¬¸ ---
st.title("ğŸ¯ ë…¼ë¬¸ ê²€ìƒ‰ê¸°")
st.info(
    f"**[ì•ˆë‚´]** 'High Impact' ì—¬ë¶€ëŠ” Google ê²€ìƒ‰ì„ í†µí•´ ì¶”ì •í•œ IFê°€ **{HIGH_IMPACT_THRESHOLD} ì´ìƒ**ì¸ì§€ ì—¬ë¶€ë¡œ íŒë‹¨í•©ë‹ˆë‹¤. "
    "ì´ ê³¼ì •ì€ **ë¶€ì •í™•í•  ìˆ˜ ìˆìœ¼ë©°, Googleì— ì˜í•´ ì°¨ë‹¨**ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
)

with st.form(key='search_form'):
    keyword = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", placeholder="ì˜ˆ: quantum computing")
    num_results = st.slider("ê²€ìƒ‰í•  ë…¼ë¬¸ ìˆ˜", 1, 10, 5, help="Google ê²€ìƒ‰ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•´ ìµœëŒ€ 10ê°œê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    submit_button = st.form_submit_button(label='ê²€ìƒ‰')

if submit_button and keyword:
    with st.spinner("ë…¼ë¬¸ ê²€ìƒ‰ ë° High Impact ì—¬ë¶€ í™•ì¸ ì¤‘..."):
        try:
            search_query = scholarly.search_pubs(keyword)
            results = []

            for i, pub in enumerate(search_query):
                if i >= num_results: break
                time.sleep(1) # Google ê²€ìƒ‰ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ì§€ì—°
                
                bib = pub.get('bib', {})
                venue = bib.get('venue', 'N/A')
                
                high_impact_status = check_if_high_impact(venue)
                
                results.append({
                    "ì œëª© (Title)": bib.get('title', 'N/A'),
                    "ì €ë„ (Venue)": venue,
                    "ì—°ë„ (Year)": bib.get('pub_year', 'N/A'),
                    "High Impact": high_impact_status,
                    "í”¼ì¸ìš© ìˆ˜": pub.get('num_citations', 0),
                    "ì €ì (Authors)": ", ".join(bib.get('author', ['N/A'])),
                    "ë…¼ë¬¸ ë§í¬": pub.get('pub_url', '#'),
                })

            if not results:
                st.warning("ê²€ìƒ‰ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.success("âœ… ê²€ìƒ‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                df = pd.DataFrame(results)
                
                # ì»¬ëŸ¼ ìˆœì„œë¥¼ ë” ë³´ê¸° ì¢‹ê²Œ ì¬ë°°ì¹˜
                df = df[["ì œëª© (Title)", "ì €ë„ (Venue)", "ì—°ë„ (Year)", "High Impact", "í”¼ì¸ìš© ìˆ˜", "ì €ì (Authors)", "ë…¼ë¬¸ ë§í¬"]]
                
                st.dataframe(
                    df, use_container_width=True,
                    column_config={"ë…¼ë¬¸ ë§í¬": st.column_config.LinkColumn("Link", display_text="ğŸ”—")},
                    hide_index=True)
                
                st.download_button(
                    label="ğŸ“„ CSV ë‹¤ìš´ë¡œë“œ",
                    data=convert_to_csv(df),
                    file_name=f'search_{keyword.replace(" ", "_")}.csv',
                    mime='text/csv'
                )

        except Exception as e:
            st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
