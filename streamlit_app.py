import streamlit as st
import pandas as pd
from scholarly import scholarly
import io
import requests
from bs4 import BeautifulSoup
import re
import time

# ----------------------------------
# í˜ì´ì§€ ì„¤ì •
# ----------------------------------
st.set_page_config(
    page_title="ë…¼ë¬¸ ê²€ìƒ‰ê¸° (Google IF ìŠ¤í¬ë ˆì´í¼)",
    page_icon="ğŸ§ª",
    layout="wide",
)

# ----------------------------------
# Google ê²€ìƒ‰ ìŠ¤í¬ë ˆì´í•‘ í•¨ìˆ˜
# ----------------------------------
@st.cache_data(ttl=3600)  # ê²°ê³¼ë¥¼ 1ì‹œê°„ ë™ì•ˆ ìºì‹œ
def get_if_from_google_search(journal_name: str):
    """
    Google ê²€ìƒ‰ì„ í†µí•´ ì €ë„ì˜ Impact Factorë¥¼ ìŠ¤í¬ë ˆì´í•‘í•©ë‹ˆë‹¤.
    **ë§¤ìš° ë¶ˆì•ˆì •í•˜ë©° ì‹¤í—˜ì ì¸ ê¸°ëŠ¥ì…ë‹ˆë‹¤.**
    """
    if not journal_name:
        return "N/A"

    try:
        # ê²€ìƒ‰ì–´ ìƒì„± (ì €ë„ ì´ë¦„ê³¼ "impact factor"ë¥¼ í•¨ê»˜ ê²€ìƒ‰)
        query = f'"{journal_name}" journal impact factor'
        url = f"https://www.google.com/search?q={query}"
        
        # Google ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•œ User-Agent ì„¤ì •
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        
        # Googleì´ ìš”ì²­ì„ ì°¨ë‹¨í–ˆëŠ”ì§€ í™•ì¸ (ì‘ë‹µ ë‚´ìš©ìœ¼ë¡œ íŒë‹¨)
        if "Our systems have detected unusual traffic" in response.text:
            return "Scraping Blocked"
        
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        # í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ IF íŒ¨í„´ ê²€ìƒ‰
        page_text = soup.get_text()
        
        # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ "Impact Factor: 12.345" ì™€ ê°™ì€ íŒ¨í„´ ì°¾ê¸°
        # (?: ... ) ëŠ” non-capturing group ì…ë‹ˆë‹¤.
        pattern = r"(?:impact factor|if)\s*[:\-]?\s*(\d{1,3}\.\d{1,3})"
        match = re.search(pattern, page_text, re.IGNORECASE)
        
        if match:
            return match.group(1) # ì²« ë²ˆì§¸ ìº¡ì²˜ ê·¸ë£¹ (ìˆ«ì ë¶€ë¶„) ë°˜í™˜
        else:
            return "Not Found"

    except requests.exceptions.RequestException:
        return "Network Error"
    except Exception:
        return "Parsing Error"

# ----------------------------------
# ë°ì´í„° ë³€í™˜ í•¨ìˆ˜
# ----------------------------------
@st.cache_data
def to_excel(df: pd.DataFrame):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Sheet1')
    return output.getvalue()

@st.cache_data
def to_csv(df: pd.DataFrame):
    return df.to_csv(index=False).encode('utf-8-sig')


# ----------------------------------
# Streamlit ì•± UI êµ¬ì„±
# ----------------------------------
st.title("ğŸ§ª ë…¼ë¬¸ ê²€ìƒ‰ê¸° (ì‹¤í—˜ì  IF ìŠ¤í¬ë ˆì´í¼)")
st.warning(
    "**[ì¤‘ìš”] ì•ˆë‚´:** ì´ ì•±ì€ **Google ê²€ìƒ‰**ì„ í†µí•´ ì €ë„ì˜ Impact Factorë¥¼ **ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì •**í•©ë‹ˆë‹¤. "
    "ì´ ë°©ë²•ì€ ì•„ë˜ì™€ ê°™ì€ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.\n"
    "1. Googleì˜ ì •ì±…ì— ì˜í•´ **ê²€ìƒ‰ì´ ìì£¼ ì°¨ë‹¨**ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ('Scraping Blocked' ì˜¤ë¥˜)\n"
    "2. í‘œì‹œë˜ëŠ” ìˆ˜ì¹˜ëŠ” **ì •í™•í•˜ì§€ ì•Šê±°ë‚˜ ì˜¤ë˜ëœ ì •ë³´**ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
    "3. ê²€ìƒ‰ ì†ë„ê°€ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤."
)

with st.form(key='search_form'):
    keyword = st.text_input("**ğŸ‘‰ ê²€ìƒ‰ì–´(Keyword)ë¥¼ ì…ë ¥í•˜ì„¸ìš”**", placeholder="ì˜ˆ: nature machine intelligence")
    num_results = st.number_input("**ğŸ‘‰ ê²€ìƒ‰í•  ë…¼ë¬¸ ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”**", min_value=1, max_value=10, value=5, step=1,
                                  help="ì†ë„ì™€ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•´ í•œ ë²ˆì— ìµœëŒ€ 10ê°œê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    submit_button = st.form_submit_button(label='ğŸ” ê²€ìƒ‰ ì‹œì‘')

if submit_button and keyword:
    with st.spinner(f"ë…¼ë¬¸ ê²€ìƒ‰ ë° Googleì—ì„œ IF ì¶”ì • ì¤‘... (ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
        try:
            search_query = scholarly.search_pubs(keyword)
            results = []
            
            for i, pub in enumerate(search_query):
                if i >= num_results: break
                
                # Google ê²€ìƒ‰ ìš”ì²­ ì‚¬ì´ì— ì¶©ë¶„í•œ ì‹œê°„ ê°„ê²© ì£¼ê¸°
                time.sleep(1) 
                
                bib = pub.get('bib', {})
                venue = bib.get('venue', 'N/A')
                # Google Scholarê°€ ì œê³µí•˜ëŠ” ì €ë„ëª…ì€ '...'ìœ¼ë¡œ ì¶•ì•½ë˜ëŠ” ê²½ìš°ê°€ ë§ìŒ
                # ì˜ˆ: 'Nature Machine Intelligence' -> 'Nat. Mach. Intell.'
                # ì´ëŠ” ì •í™•í•œ ê²€ìƒ‰ì„ ë°©í•´í•  ìˆ˜ ìˆìŒ
                
                impact_factor = get_if_from_google_search(venue)
                
                results.append({
                    "ì œëª© (Title)": bib.get('title', 'N/A'),
                    "ì €ì (Authors)": ", ".join(bib.get('author', ['N/A'])),
                    "ì—°ë„ (Year)": bib.get('pub_year', 'N/A'),
                    "ì €ë„/ì¶œíŒë¬¼ (Venue)": venue,
                    "IF ì¶”ì •ì¹˜ (Google)": impact_factor,
                    "í”¼ì¸ìš© ìˆ˜ (Citations)": pub.get('num_citations', 0),
                    "ë…¼ë¬¸ ë§í¬": pub.get('pub_url', '#'),
                })

            df = pd.DataFrame(results)
            st.success(f"ì´ {len(df)}ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            st.dataframe(
                df, use_container_width=True,
                column_config={"ë…¼ë¬¸ ë§í¬": st.column_config.LinkColumn("ë°”ë¡œê°€ê¸°")},
                hide_index=True)
            
            st.markdown("---")
            st.subheader("ğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ")
            col1, col2 = st.columns(2)
            with col1:
                st.download_button("ğŸ“„ CSV ë‹¤ìš´ë¡œë“œ", to_csv(df), f'google_if_{keyword.replace(" ", "_")}.csv', 'text/csv')
            with col2:
                st.download_button("ğŸ“Š Excel ë‹¤ìš´ë¡œë“œ", to_excel(df), f'google_if_{keyword.replace(" ", "_")}.xlsx')

        except Exception as e:
            st.error(f"ê²€ìƒ‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            st.info("Google Scholarì˜ ìš”ì²­ì´ ì°¨ë‹¨ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

elif submit_button and not keyword:
    st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
